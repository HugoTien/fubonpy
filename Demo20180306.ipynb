{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Distance\n",
    "- pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting editdistance\n",
      "  Downloading editdistance-0.4-cp36-cp36m-win_amd64.whl\n",
      "Installing collected packages: editdistance\n",
      "Successfully installed editdistance-0.4\n"
     ]
    }
   ],
   "source": [
    "! pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import editdistance\n",
    "editdistance.eval('banana', 'bahama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'banana' # n => h\n",
    "'bahana' # n => m\n",
    "'bahama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import editdistance\n",
    "editdistance.eval('machine', 'macinae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'machine' #=> delete h\n",
    "'macine'  #=> insert a\n",
    "'macinae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = ['How to format my hard disk', 'Hard disk format problems']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(content)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取 toy 資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "Imaging databases provide storage capabilities.\n",
      "Most imaging databases safe images permanently.\n",
      "Imaging databases store data.\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = 'data/toy'\n",
    "for f in os.listdir(path):\n",
    "    print(open(os.path.join(path, f)).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a toy post about machine learning. Actually, it contains not much interesting stuff.',\n",
       " 'Imaging databases provide storage capabilities.',\n",
       " 'Most imaging databases safe images permanently.',\n",
       " 'Imaging databases store data.',\n",
       " 'Imaging databases store data. Imaging databases store data. Imaging databases store data.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = [open(os.path.join(path, f)).read() for f in os.listdir(path)]\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x25 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X= vectorizer.fit_transform(posts)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x25 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = 'imaging database qoo'\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4494897427831779"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "a = np.array([1,1,0,0,1])\n",
    "b = np.array([0,1,1,0,1])\n",
    "\n",
    "# method 1\n",
    "sum((a - b) ** 2) ** (1/2)\n",
    "\n",
    "# method 2\n",
    "#np.array([a,b])\n",
    "sp.linalg.norm(np.array([a,b]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算imaging database 與每篇文章的距離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=3.87: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=2.00: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.24: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=1.73: Imaging databases store data.\n",
      "=== Post 4 with dist=5.57: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=1.73\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = 999\n",
    "best_i = None\n",
    "num_samples = len(posts)\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X.getrow(i)\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用sklearn 的 euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.87298335,  2.        ,  2.23606798,  1.73205081,  5.56776436]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "ed = euclidean_distances(new_post_vec, X)\n",
    "ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imaging databases store data.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos = ed.argsort()[0][0]\n",
    "pos = ed.argsort().flatten()[0]\n",
    "posts[pos]\n",
    "#posts[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更改距離計算方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8284271247461903"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "a = np.array([1,1,0,0,1])\n",
    "b = np.array([0,1,1,0,1])\n",
    "c = np.array([1,1,0,0,1,0,0,0,0,0,0,0])\n",
    "d = np.array([1,1,0,0,1,1,1,1,1,1])\n",
    "a\n",
    "sp.linalg.norm([d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    v1_normalized  = v1 / sp.linalg.norm(v1) \n",
    "    v2_normalized  = v2 / sp.linalg.norm(v2)\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.05: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=1.09: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=1.00: Imaging databases store data.\n",
      "=== Post 4 with dist=1.00: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=1.00\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = 999\n",
    "best_i = None\n",
    "num_samples = len(posts)\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X.getrow(i).toarray()\n",
    "    d = dist(post_vec, new_post_vec.toarray())\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually',\n",
       " 'capabilities',\n",
       " 'contains',\n",
       " 'data',\n",
       " 'databases',\n",
       " 'images',\n",
       " 'imaging',\n",
       " 'interesting',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'permanently',\n",
       " 'post',\n",
       " 'provide',\n",
       " 'safe',\n",
       " 'storage',\n",
       " 'store',\n",
       " 'stuff',\n",
       " 'toy']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = CountVectorizer()\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(posts)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afterwards',\n",
       " 'hundred',\n",
       " 'them',\n",
       " 'more',\n",
       " 'name',\n",
       " 'against',\n",
       " 'beside',\n",
       " 'indeed',\n",
       " 'the',\n",
       " 'about']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.get_stop_words())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安裝 nltk\n",
    "- pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphic'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "s.stem('graphics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imag'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('imaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imag'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('imagination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('imagine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        # retunr lambda doc : (analyzer(doc))\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'safe', 'storag', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x17 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = 'imaging database qoo'\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    v1_normalized  = v1 / sp.linalg.norm(v1) \n",
    "    v2_normalized  = v2 / sp.linalg.norm(v2)\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.63: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.63\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = 999\n",
    "best_i = None\n",
    "num_samples = len(posts)\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X.getrow(i).toarray()\n",
    "    d = dist(post_vec, new_post_vec.toarray())\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.26726124,  0.26726124,  0.        ,  0.26726124,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.26726124,  0.26726124,\n",
       "         0.26726124,  0.26726124,  0.26726124,  0.        ,  0.26726124,\n",
       "         0.26726124,  0.        ,  0.26726124,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.26726124,  0.26726124,  0.26726124],\n",
       "       [ 0.        ,  0.        ,  0.52451722,  0.        ,  0.        ,\n",
       "         0.29550385,  0.        ,  0.29550385,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.52451722,  0.        ,\n",
       "         0.52451722,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.26169047,  0.46449871,  0.26169047,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.46449871,  0.        ,\n",
       "         0.        ,  0.46449871,  0.        ,  0.        ,  0.46449871,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.57974759,\n",
       "         0.40483667,  0.        ,  0.40483667,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.57974759,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.57974759,\n",
       "         0.40483667,  0.        ,  0.40483667,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.57974759,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(posts)\n",
    "print(vectorizer.get_feature_names())\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=1,stop_words='english')\n",
    "X = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actual',\n",
       " 'capabl',\n",
       " 'contain',\n",
       " 'data',\n",
       " 'databas',\n",
       " 'imag',\n",
       " 'interest',\n",
       " 'learn',\n",
       " 'machin',\n",
       " 'perman',\n",
       " 'post',\n",
       " 'provid',\n",
       " 'safe',\n",
       " 'storag',\n",
       " 'store',\n",
       " 'stuff',\n",
       " 'toy']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x17 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = 'imaging database qoo'\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.08: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.92: Imaging databases store data.\n",
      "=== Post 4 with dist=0.92: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.86\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = 999\n",
    "best_i = None\n",
    "num_samples = len(posts)\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X.getrow(i).toarray()\n",
    "    d = dist(post_vec, new_post_vec.toarray())\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用cosine distance 計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "cs = cosine_distances(new_post_vec,X)\n",
    "cs.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 同義字處理 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['柯文哲 為了 大巨蛋 一事 找 趙藤雄 算帳', '柯P 將 不在 大巨蛋 舉辦 世運會']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '柯文哲/柯P'\n",
    "\n",
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "\n",
    "a = ['柯文哲為了大巨蛋一事找趙藤雄算帳', '柯P將不在大巨蛋舉辦世運會']\n",
    "\n",
    "corpus = [' '.join(jieba.cut(s)) for s in a]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一事', '不在', '世運會', '大巨蛋', '柯p', '柯文哲', '為了', '算帳', '舉辦', '趙藤雄']\n",
      "[[1 0 0 1 0 1 1 1 0 1]\n",
      " [0 1 1 1 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立同義詞字典 (從維基百科取得同義詞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'柯文哲/柯P/KP'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "term = '柯文哲'\n",
    "res = requests.get('https://zh.wikipedia.org/wiki/{}'.format(term))\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "'/'.join([w.text for w in soup.select_one('.mw-parser-output p').select('b')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將同義詞輸入成Python 字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kp': '柯文哲', '柯p': '柯文哲', '特朗普': '川普', '趙大雄': '趙藤雄'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_dic = {}\n",
    "for s in open('synonym.txt'):\n",
    "    synonym = s.strip().split('/')\n",
    "    for w in synonym[1:]:\n",
    "        synonym_dic[w.lower()]  = synonym[0]\n",
    "synonym_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生 SynonymCountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "\n",
    "class SynonymCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SynonymCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (synonym_dic.get(w, w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一事', '不在', '世運會', '大巨蛋', '柯文哲', '為了', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = SynonymCountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 1, 1, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定中文停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = ['為了', '一事', '不在']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['世運會', '大巨蛋', '柯文哲', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = SynonymCountVectorizer(stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 英文資料分類 (垃圾郵件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/email3/easy_ham/01251.793e5c04967cb90191e805dfa619c55a', 'r') as f:\n",
    "    msg = f.readlines()\n",
    "    first_blank_index = msg.index('\\n')\n",
    "    msg = msg[(first_blank_index + 1): ]\n",
    "    #print(''.join(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?open\n",
    "\n",
    "def get_msg(path):\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        msg = f.readlines()\n",
    "        first_blank_index = msg.index('\\n')\n",
    "        msg = msg[(first_blank_index + 1): ]\n",
    "        return ''.join(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Matthias Haase wrote:\\n> RH ships the code with the bytecode hinter disabled which makes \\n> non-AA fonts really ugly.\\n> This reqiures only a small change for include/freetype/config/ftoption.h,\\n> it is very well documented.\\n\\nRed Hat 8.0 ships with the bytecode hinter enabled; I think 7.3 may have \\nas well.\\n\\nThe enabling change to \"ftoption.h\" is made by Red Hat\\'s SRPM before \\nbuilding.  Take a look at \"freetype-2.1.1-enable-ft2-bci.patch\" from the \\nSRPM; it\\'s pretty clear that this does exactly what needs to be done.\\n\\nSo if your fonts look ugly, lack of bytecode hinting is *not* the cause.\\n\\n\\n_______________________________________________\\nRPM-List mailing list <RPM-List@freshrpms.net>\\nhttp://lists.freshrpms.net/mailman/listinfo/rpm-list\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_msg('data/email3/easy_ham/01251.793e5c04967cb90191e805dfa619c55a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filelist = os.listdir('data/email3/easy_ham')\n",
    "filelist = [f for f in filelist if f != 'cmds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'data/email3/easy_ham/'\n",
    "\n",
    "def get_msgdir(path):\n",
    "    msg_ary = []\n",
    "    for f in os.listdir(path):\n",
    "        if f != 'cmds':\n",
    "            msg_ary.append(get_msg(os.path.join(path, f)))\n",
    "    return msg_ary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_spam_messages    = get_msgdir('data/email3/spam')\n",
    "train_easyham_messages = get_msgdir('data/email3/easy_ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_easyham_messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_spam_messages    = get_msgdir('data/email3/spam_2')\n",
    "test_easyham_messages = get_msgdir('data/email3/easy_ham_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用re 去除掉特殊字元\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iabc456789I'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_spam_messages[0]\n",
    "import re\n",
    "a = 'Iabc3D4563D789I'\n",
    "re.sub('3D', '', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Save up to 70% on Life Insurance. Why Spend More Than You Have To? Life Quote Savings Ensurin= g your family's financial security is very important. Life Quote Savings ma= kes buying life insurance simple and affordable. We Provide FREE Access = to The Very Best Companies and The Lowest Rates. Life Quote Savings is FAST, EAS= Y and SAVES you money! Let us help you get started with the best val= ues in the country on new coverage. You can SAVE hundreds or even tho= usands of dollars by requesting a FREE quote from Lifequote Savings. = Our service will take you less than 5 minutes to complete. Shop an= d compare. SAVE up to 70% on all types of Life insurance! Click Here For Your= Free Quote! Protecting your family is the best investment you'll eve= r make! If you are in receipt of this= email in error and/or wish to be removed from our list, PLEASE CLICK HERE AND TYPE = REMOVE. If you reside in any state which prohibits e-mail solicitations for insuran= ce, please disregard this email.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = ' '.join(re.sub('<(.|\\n)*?>', '',train_spam_messages[0]).split())\n",
    "msg = re.sub('&\\w+;', ' ', msg)\n",
    "msg = re.sub('_+', '_', msg)\n",
    "\n",
    "msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[w for w in msg.lower().split() if re.search('[a-zA-Z]', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"save up to on life insurance why spend more than you have to life quote savings ensurin your family's financial security is very important life quote savings ma kes buying life insurance simple and affordable we provide free access to the very best companies and the lowest rates life quote savings is fast eas and saves you money let us help you get started with the best val ues in the country on new coverage you can save hundreds or even tho usands of dollars by requesting free quote from lifequote savings our service will take you less than minutes to complete shop an compare save up to on all types of life insurance click here for your free quote protecting your family is the best investment you'll eve make if you are in receipt of this email in error and or wish to be removed from our list please click here and type remove if you reside in any state which prohibits e-mail solicitations for insuran ce please disregard this email\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([w for w in re.split('=|\\.|!| +|,|\\?|/',msg.lower()) \\\n",
    " if re.search('[a-zA-Z]', w) and len(w) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立資料清理函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_msg_words(msg):\n",
    "    msg = re.sub('3D', '', msg)\n",
    "    \n",
    "    msg = re.sub('<(.|\\n)*?>', ' ', msg)\n",
    "    msg = re.sub('&\\w+;', ' ', msg)\n",
    "\n",
    "    msg = re.sub('_+', '_', msg)\n",
    "    msg = ' '.join(msg.split())\n",
    "    msg = ' '.join([w for w in re.split('=|\\.|!| +|,|\\?|/',msg.lower()) \\\n",
    "         if re.search('[a-zA-Z]', w) and len(w) > 1])\n",
    "    return msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"save up to on life insurance why spend more than you have to life quote savings ensurin your family's financial security is very important life quote savings ma kes buying life insurance simple and affordable we provide free access to the very best companies and the lowest rates life quote savings is fast eas and saves you money let us help you get started with the best val ues in the country on new coverage you can save hundreds or even tho usands of dollars by requesting free quote from lifequote savings our service will take you less than minutes to complete shop an compare save up to on all types of life insurance click here for your free quote protecting your family is the best investment you'll eve make if you are in receipt of this email in error and or wish to be removed from our list please click here and type remove if you reside in any state which prohibits e-mail solicitations for insuran ce please disregard this email\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_msg_words(train_spam_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_spam = [get_msg_words(w) for w in train_spam_messages]\n",
    "train_set_ham = [get_msg_words(w) for w in train_easyham_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "tags   = []\n",
    "\n",
    "for message in train_set_spam:\n",
    "    corpus.append(message)\n",
    "    tags.append('spam')\n",
    "\n",
    "for message in train_set_ham:\n",
    "    corpus.append(message)\n",
    "    tags.append('ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus), len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3000x48946 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 272752 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = StemmedTfidfVectorizer(stop_words='english')\n",
    "vectorizer = StemmedCountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將資料分成訓練與測試資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_tag, test_tag = train_test_split(X, tags, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立分類模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha = 0.1)\n",
    "clf.fit(train_data, train_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 驗證預測準確度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98282828282828283"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_tag, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[817,   1],\n",
       "       [ 16, 156]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(clf.classes_)\n",
    "confusion_matrix(test_tag, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
