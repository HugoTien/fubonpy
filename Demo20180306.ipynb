{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Distance\n",
    "- pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting editdistance\n",
      "  Downloading editdistance-0.4-cp36-cp36m-win_amd64.whl\n",
      "Installing collected packages: editdistance\n",
      "Successfully installed editdistance-0.4\n"
     ]
    }
   ],
   "source": [
    "! pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import editdistance\n",
    "editdistance.eval('banana', 'bahama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'banana' # n => h\n",
    "'bahana' # n => m\n",
    "'bahama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import editdistance\n",
    "editdistance.eval('machine', 'macinae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'machine' #=> delete h\n",
    "'macine'  #=> insert a\n",
    "'macinae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = ['How to format my hard disk', 'Hard disk format problems']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(content)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取 toy 資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "Imaging databases provide storage capabilities.\n",
      "Most imaging databases safe images permanently.\n",
      "Imaging databases store data.\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = 'data/toy'\n",
    "for f in os.listdir(path):\n",
    "    print(open(os.path.join(path, f)).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a toy post about machine learning. Actually, it contains not much interesting stuff.',\n",
       " 'Imaging databases provide storage capabilities.',\n",
       " 'Most imaging databases safe images permanently.',\n",
       " 'Imaging databases store data.',\n",
       " 'Imaging databases store data. Imaging databases store data. Imaging databases store data.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = [open(os.path.join(path, f)).read() for f in os.listdir(path)]\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x25 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X= vectorizer.fit_transform(posts)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x25 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = 'imaging database qoo'\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4494897427831779"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "a = np.array([1,1,0,0,1])\n",
    "b = np.array([0,1,1,0,1])\n",
    "\n",
    "# method 1\n",
    "sum((a - b) ** 2) ** (1/2)\n",
    "\n",
    "# method 2\n",
    "#np.array([a,b])\n",
    "sp.linalg.norm(np.array([a,b]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算imaging database 與每篇文章的距離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=3.87: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=2.00: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.24: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=1.73: Imaging databases store data.\n",
      "=== Post 4 with dist=5.57: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=1.73\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = 999\n",
    "best_i = None\n",
    "num_samples = len(posts)\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X.getrow(i)\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用sklearn 的 euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.87298335,  2.        ,  2.23606798,  1.73205081,  5.56776436]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "ed = euclidean_distances(new_post_vec, X)\n",
    "ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imaging databases store data.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos = ed.argsort()[0][0]\n",
    "pos = ed.argsort().flatten()[0]\n",
    "posts[pos]\n",
    "#posts[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更改距離計算方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8284271247461903"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "a = np.array([1,1,0,0,1])\n",
    "b = np.array([0,1,1,0,1])\n",
    "c = np.array([1,1,0,0,1,0,0,0,0,0,0,0])\n",
    "d = np.array([1,1,0,0,1,1,1,1,1,1])\n",
    "a\n",
    "sp.linalg.norm([d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    v1_normalized  = v1 / sp.linalg.norm(v1) \n",
    "    v2_normalized  = v2 / sp.linalg.norm(v2)\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.05: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=1.09: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=1.00: Imaging databases store data.\n",
      "=== Post 4 with dist=1.00: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=1.00\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = 999\n",
    "best_i = None\n",
    "num_samples = len(posts)\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X.getrow(i).toarray()\n",
    "    d = dist(post_vec, new_post_vec.toarray())\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually',\n",
       " 'capabilities',\n",
       " 'contains',\n",
       " 'data',\n",
       " 'databases',\n",
       " 'images',\n",
       " 'imaging',\n",
       " 'interesting',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'permanently',\n",
       " 'post',\n",
       " 'provide',\n",
       " 'safe',\n",
       " 'storage',\n",
       " 'store',\n",
       " 'stuff',\n",
       " 'toy']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = CountVectorizer()\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(posts)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afterwards',\n",
       " 'hundred',\n",
       " 'them',\n",
       " 'more',\n",
       " 'name',\n",
       " 'against',\n",
       " 'beside',\n",
       " 'indeed',\n",
       " 'the',\n",
       " 'about']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.get_stop_words())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安裝 nltk\n",
    "- pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphic'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "s.stem('graphics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imag'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('imaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imag'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('imagination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('imagine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        # retunr lambda doc : (analyzer(doc))\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'safe', 'storag', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x17 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = 'imaging database qoo'\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    v1_normalized  = v1 / sp.linalg.norm(v1) \n",
    "    v2_normalized  = v2 / sp.linalg.norm(v2)\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.63: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.63\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = 999\n",
    "best_i = None\n",
    "num_samples = len(posts)\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X.getrow(i).toarray()\n",
    "    d = dist(post_vec, new_post_vec.toarray())\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.26726124,  0.26726124,  0.        ,  0.26726124,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.26726124,  0.26726124,\n",
       "         0.26726124,  0.26726124,  0.26726124,  0.        ,  0.26726124,\n",
       "         0.26726124,  0.        ,  0.26726124,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.26726124,  0.26726124,  0.26726124],\n",
       "       [ 0.        ,  0.        ,  0.52451722,  0.        ,  0.        ,\n",
       "         0.29550385,  0.        ,  0.29550385,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.52451722,  0.        ,\n",
       "         0.52451722,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.26169047,  0.46449871,  0.26169047,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.46449871,  0.        ,\n",
       "         0.        ,  0.46449871,  0.        ,  0.        ,  0.46449871,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.57974759,\n",
       "         0.40483667,  0.        ,  0.40483667,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.57974759,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.57974759,\n",
       "         0.40483667,  0.        ,  0.40483667,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.57974759,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(posts)\n",
    "print(vectorizer.get_feature_names())\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=1,stop_words='english')\n",
    "X = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actual',\n",
       " 'capabl',\n",
       " 'contain',\n",
       " 'data',\n",
       " 'databas',\n",
       " 'imag',\n",
       " 'interest',\n",
       " 'learn',\n",
       " 'machin',\n",
       " 'perman',\n",
       " 'post',\n",
       " 'provid',\n",
       " 'safe',\n",
       " 'storag',\n",
       " 'store',\n",
       " 'stuff',\n",
       " 'toy']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x17 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = 'imaging database qoo'\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.08: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.92: Imaging databases store data.\n",
      "=== Post 4 with dist=0.92: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.86\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = 999\n",
    "best_i = None\n",
    "num_samples = len(posts)\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X.getrow(i).toarray()\n",
    "    d = dist(post_vec, new_post_vec.toarray())\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用cosine distance 計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "cs = cosine_distances(new_post_vec,X)\n",
    "cs.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 同義字處理 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['柯文哲 為了 大巨蛋 一事 找 趙藤雄 算帳', '柯P 將 不在 大巨蛋 舉辦 世運會']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '柯文哲/柯P'\n",
    "\n",
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "\n",
    "a = ['柯文哲為了大巨蛋一事找趙藤雄算帳', '柯P將不在大巨蛋舉辦世運會']\n",
    "\n",
    "corpus = [' '.join(jieba.cut(s)) for s in a]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一事', '不在', '世運會', '大巨蛋', '柯p', '柯文哲', '為了', '算帳', '舉辦', '趙藤雄']\n",
      "[[1 0 0 1 0 1 1 1 0 1]\n",
      " [0 1 1 1 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立同義詞字典 (從維基百科取得同義詞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'柯文哲/柯P/KP'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "term = '柯文哲'\n",
    "res = requests.get('https://zh.wikipedia.org/wiki/{}'.format(term))\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "'/'.join([w.text for w in soup.select_one('.mw-parser-output p').select('b')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將同義詞輸入成Python 字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kp': '柯文哲', '柯p': '柯文哲', '特朗普': '川普', '趙大雄': '趙藤雄'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_dic = {}\n",
    "for s in open('synonym.txt'):\n",
    "    synonym = s.strip().split('/')\n",
    "    for w in synonym[1:]:\n",
    "        synonym_dic[w.lower()]  = synonym[0]\n",
    "synonym_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生 SynonymCountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "\n",
    "class SynonymCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SynonymCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (synonym_dic.get(w, w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一事', '不在', '世運會', '大巨蛋', '柯文哲', '為了', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = SynonymCountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 1, 1, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定中文停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = ['為了', '一事', '不在']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['世運會', '大巨蛋', '柯文哲', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = SynonymCountVectorizer(stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 英文資料分類 (垃圾郵件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/email3/easy_ham/01251.793e5c04967cb90191e805dfa619c55a', 'r') as f:\n",
    "    msg = f.readlines()\n",
    "    first_blank_index = msg.index('\\n')\n",
    "    msg = msg[(first_blank_index + 1): ]\n",
    "    #print(''.join(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?open\n",
    "\n",
    "def get_msg(path):\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        msg = f.readlines()\n",
    "        first_blank_index = msg.index('\\n')\n",
    "        msg = msg[(first_blank_index + 1): ]\n",
    "        return ''.join(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Matthias Haase wrote:\\n> RH ships the code with the bytecode hinter disabled which makes \\n> non-AA fonts really ugly.\\n> This reqiures only a small change for include/freetype/config/ftoption.h,\\n> it is very well documented.\\n\\nRed Hat 8.0 ships with the bytecode hinter enabled; I think 7.3 may have \\nas well.\\n\\nThe enabling change to \"ftoption.h\" is made by Red Hat\\'s SRPM before \\nbuilding.  Take a look at \"freetype-2.1.1-enable-ft2-bci.patch\" from the \\nSRPM; it\\'s pretty clear that this does exactly what needs to be done.\\n\\nSo if your fonts look ugly, lack of bytecode hinting is *not* the cause.\\n\\n\\n_______________________________________________\\nRPM-List mailing list <RPM-List@freshrpms.net>\\nhttp://lists.freshrpms.net/mailman/listinfo/rpm-list\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_msg('data/email3/easy_ham/01251.793e5c04967cb90191e805dfa619c55a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filelist = os.listdir('data/email3/easy_ham')\n",
    "filelist = [f for f in filelist if f != 'cmds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'data/email3/easy_ham/'\n",
    "\n",
    "def get_msgdir(path):\n",
    "    msg_ary = []\n",
    "    for f in os.listdir(path):\n",
    "        if f != 'cmds':\n",
    "            msg_ary.append(get_msg(os.path.join(path, f)))\n",
    "    return msg_ary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_spam_messages    = get_msgdir('data/email3/spam')\n",
    "train_easyham_messages = get_msgdir('data/email3/easy_ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_easyham_messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_spam_messages    = get_msgdir('data/email3/spam_2')\n",
    "test_easyham_messages = get_msgdir('data/email3/easy_ham_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用re 去除掉特殊字元\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iabc456789I'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_spam_messages[0]\n",
    "import re\n",
    "a = 'Iabc3D4563D789I'\n",
    "re.sub('3D', '', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Save up to 70% on Life Insurance. Why Spend More Than You Have To? Life Quote Savings Ensurin= g your family's financial security is very important. Life Quote Savings ma= kes buying life insurance simple and affordable. We Provide FREE Access = to The Very Best Companies and The Lowest Rates. Life Quote Savings is FAST, EAS= Y and SAVES you money! Let us help you get started with the best val= ues in the country on new coverage. You can SAVE hundreds or even tho= usands of dollars by requesting a FREE quote from Lifequote Savings. = Our service will take you less than 5 minutes to complete. Shop an= d compare. SAVE up to 70% on all types of Life insurance! Click Here For Your= Free Quote! Protecting your family is the best investment you'll eve= r make! If you are in receipt of this= email in error and/or wish to be removed from our list, PLEASE CLICK HERE AND TYPE = REMOVE. If you reside in any state which prohibits e-mail solicitations for insuran= ce, please disregard this email.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = ' '.join(re.sub('<(.|\\n)*?>', '',train_spam_messages[0]).split())\n",
    "msg = re.sub('&\\w+;', ' ', msg)\n",
    "msg = re.sub('_+', '_', msg)\n",
    "\n",
    "msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[w for w in msg.lower().split() if re.search('[a-zA-Z]', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"save up to on life insurance why spend more than you have to life quote savings ensurin your family's financial security is very important life quote savings ma kes buying life insurance simple and affordable we provide free access to the very best companies and the lowest rates life quote savings is fast eas and saves you money let us help you get started with the best val ues in the country on new coverage you can save hundreds or even tho usands of dollars by requesting free quote from lifequote savings our service will take you less than minutes to complete shop an compare save up to on all types of life insurance click here for your free quote protecting your family is the best investment you'll eve make if you are in receipt of this email in error and or wish to be removed from our list please click here and type remove if you reside in any state which prohibits e-mail solicitations for insuran ce please disregard this email\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([w for w in re.split('=|\\.|!| +|,|\\?|/',msg.lower()) \\\n",
    " if re.search('[a-zA-Z]', w) and len(w) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立資料清理函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_msg_words(msg):\n",
    "    msg = re.sub('3D', '', msg)\n",
    "    \n",
    "    msg = re.sub('<(.|\\n)*?>', ' ', msg)\n",
    "    msg = re.sub('&\\w+;', ' ', msg)\n",
    "\n",
    "    msg = re.sub('_+', '_', msg)\n",
    "    msg = ' '.join(msg.split())\n",
    "    msg = ' '.join([w for w in re.split('=|\\.|!| +|,|\\?|/',msg.lower()) \\\n",
    "         if re.search('[a-zA-Z]', w) and len(w) > 1])\n",
    "    return msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"save up to on life insurance why spend more than you have to life quote savings ensurin your family's financial security is very important life quote savings ma kes buying life insurance simple and affordable we provide free access to the very best companies and the lowest rates life quote savings is fast eas and saves you money let us help you get started with the best val ues in the country on new coverage you can save hundreds or even tho usands of dollars by requesting free quote from lifequote savings our service will take you less than minutes to complete shop an compare save up to on all types of life insurance click here for your free quote protecting your family is the best investment you'll eve make if you are in receipt of this email in error and or wish to be removed from our list please click here and type remove if you reside in any state which prohibits e-mail solicitations for insuran ce please disregard this email\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_msg_words(train_spam_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_spam = [get_msg_words(w) for w in train_spam_messages]\n",
    "train_set_ham = [get_msg_words(w) for w in train_easyham_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Sklearn 分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "tags   = []\n",
    "\n",
    "for message in train_set_spam:\n",
    "    corpus.append(message)\n",
    "    tags.append('spam')\n",
    "\n",
    "for message in train_set_ham:\n",
    "    corpus.append(message)\n",
    "    tags.append('ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus), len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3000x48946 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 272752 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = StemmedTfidfVectorizer(stop_words='english')\n",
    "vectorizer = StemmedCountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將資料分成訓練與測試資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_tag, test_tag = train_test_split(X, tags, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立分類模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha = 0.1)\n",
    "clf.fit(train_data, train_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel = 'linear')\n",
    "clf.fit(train_data, train_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 驗證預測準確度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97070707070707074"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_tag, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[811,   7],\n",
       "       [ 22, 150]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(clf.classes_)\n",
    "confusion_matrix(test_tag, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用NLTK 分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "sw.extend(['ll', 've'])\n",
    "#print(sw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立資料清理函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def get_msg_words(msg, stopwords):\n",
    "    msg = re.sub('3D', '', msg)\n",
    "    \n",
    "    msg = re.sub('<(.|\\n)*?>', ' ', msg)\n",
    "    msg = re.sub('&\\w+;', ' ', msg)\n",
    "\n",
    "    msg = re.sub('_+', '_', msg)\n",
    "    msg_words = set(wordpunct_tokenize(msg.replace('=\\n', '').lower()))\n",
    "\n",
    "    msg_words = msg_words.difference(stopwords)\n",
    "    msg_words = [w for w in msg_words if re.search('[a-zA-Z]', w) and len(w) > 1]\n",
    "    return msg_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取出特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_indicator(msg):\n",
    "    features = {}\n",
    "    msg_words = get_msg_words(msg, sw)\n",
    "    for  w in msg_words:\n",
    "        features[w] = True\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access': True,\n",
       " 'affordable': True,\n",
       " 'best': True,\n",
       " 'buying': True,\n",
       " 'click': True,\n",
       " 'companies': True,\n",
       " 'compare': True,\n",
       " 'complete': True,\n",
       " 'country': True,\n",
       " 'coverage': True,\n",
       " 'disregard': True,\n",
       " 'dollars': True,\n",
       " 'easy': True,\n",
       " 'email': True,\n",
       " 'ensuring': True,\n",
       " 'error': True,\n",
       " 'even': True,\n",
       " 'ever': True,\n",
       " 'family': True,\n",
       " 'fast': True,\n",
       " 'financial': True,\n",
       " 'free': True,\n",
       " 'get': True,\n",
       " 'help': True,\n",
       " 'hundreds': True,\n",
       " 'important': True,\n",
       " 'insurance': True,\n",
       " 'investment': True,\n",
       " 'less': True,\n",
       " 'let': True,\n",
       " 'life': True,\n",
       " 'lifequote': True,\n",
       " 'list': True,\n",
       " 'lowest': True,\n",
       " 'mail': True,\n",
       " 'make': True,\n",
       " 'makes': True,\n",
       " 'minutes': True,\n",
       " 'money': True,\n",
       " 'new': True,\n",
       " 'please': True,\n",
       " 'prohibits': True,\n",
       " 'protecting': True,\n",
       " 'provide': True,\n",
       " 'quote': True,\n",
       " 'rates': True,\n",
       " 'receipt': True,\n",
       " 'remove': True,\n",
       " 'removed': True,\n",
       " 'requesting': True,\n",
       " 'reside': True,\n",
       " 'save': True,\n",
       " 'saves': True,\n",
       " 'savings': True,\n",
       " 'security': True,\n",
       " 'service': True,\n",
       " 'shop': True,\n",
       " 'simple': True,\n",
       " 'solicitations': True,\n",
       " 'spend': True,\n",
       " 'started': True,\n",
       " 'state': True,\n",
       " 'take': True,\n",
       " 'thousands': True,\n",
       " 'type': True,\n",
       " 'types': True,\n",
       " 'us': True,\n",
       " 'values': True,\n",
       " 'wish': True}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indicator(train_spam_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features_from_messages(messages, label, feature_extractor):\n",
    "    features_labels = []\n",
    "    for msg in messages:\n",
    "        features = feature_extractor(msg)\n",
    "        features_labels.append((features, label))\n",
    "    return features_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立訓練資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_spam = features_from_messages(train_spam_messages, 'spam', word_indicator)\n",
    "train_ham  = features_from_messages(train_easyham_messages, 'ham', word_indicator)\n",
    "train_set = train_spam + train_ham\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'access': True,\n",
       "  'affordable': True,\n",
       "  'best': True,\n",
       "  'buying': True,\n",
       "  'click': True,\n",
       "  'companies': True,\n",
       "  'compare': True,\n",
       "  'complete': True,\n",
       "  'country': True,\n",
       "  'coverage': True,\n",
       "  'disregard': True,\n",
       "  'dollars': True,\n",
       "  'easy': True,\n",
       "  'email': True,\n",
       "  'ensuring': True,\n",
       "  'error': True,\n",
       "  'even': True,\n",
       "  'ever': True,\n",
       "  'family': True,\n",
       "  'fast': True,\n",
       "  'financial': True,\n",
       "  'free': True,\n",
       "  'get': True,\n",
       "  'help': True,\n",
       "  'hundreds': True,\n",
       "  'important': True,\n",
       "  'insurance': True,\n",
       "  'investment': True,\n",
       "  'less': True,\n",
       "  'let': True,\n",
       "  'life': True,\n",
       "  'lifequote': True,\n",
       "  'list': True,\n",
       "  'lowest': True,\n",
       "  'mail': True,\n",
       "  'make': True,\n",
       "  'makes': True,\n",
       "  'minutes': True,\n",
       "  'money': True,\n",
       "  'new': True,\n",
       "  'please': True,\n",
       "  'prohibits': True,\n",
       "  'protecting': True,\n",
       "  'provide': True,\n",
       "  'quote': True,\n",
       "  'rates': True,\n",
       "  'receipt': True,\n",
       "  'remove': True,\n",
       "  'removed': True,\n",
       "  'requesting': True,\n",
       "  'reside': True,\n",
       "  'save': True,\n",
       "  'saves': True,\n",
       "  'savings': True,\n",
       "  'security': True,\n",
       "  'service': True,\n",
       "  'shop': True,\n",
       "  'simple': True,\n",
       "  'solicitations': True,\n",
       "  'spend': True,\n",
       "  'started': True,\n",
       "  'state': True,\n",
       "  'take': True,\n",
       "  'thousands': True,\n",
       "  'type': True,\n",
       "  'types': True,\n",
       "  'us': True,\n",
       "  'values': True,\n",
       "  'wish': True},\n",
       " 'spam')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立測試資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_spam = features_from_messages(test_spam_messages, 'spam', word_indicator)\n",
    "test_ham  = features_from_messages(test_easyham_messages, 'ham', word_indicator)\n",
    "test_set = test_spam + test_ham\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用貝氏分類建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 驗證準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Spam accuracy: 96.42%\n",
      "Test Ham accuracy: 86.70%\n"
     ]
    }
   ],
   "source": [
    "print ('Test Spam accuracy: {0:.2f}%'\n",
    "  .format(100 * nltk.classify.accuracy(classifier, test_spam)))\n",
    "\n",
    "print ('Test Ham accuracy: {0:.2f}%'\n",
    "  .format(100 * nltk.classify.accuracy(classifier, test_ham)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set accuracy: 91.56%\n"
     ]
    }
   ],
   "source": [
    "print ('Test Set accuracy: {0:.2f}%'\n",
    "  .format(100 * nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列出最有效辨認的特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                mailings = True             spam : ham    =    124.8 : 1.0\n",
      "                unwanted = True             spam : ham    =     71.6 : 1.0\n",
      "              requesting = True             spam : ham    =     64.9 : 1.0\n",
      "                mortgage = True             spam : ham    =     64.9 : 1.0\n",
      "            unsubscribed = True             spam : ham    =     62.9 : 1.0\n",
      "                 removal = True             spam : ham    =     56.0 : 1.0\n",
      "               sincerely = True             spam : ham    =     49.2 : 1.0\n",
      "             instruction = True             spam : ham    =     48.3 : 1.0\n",
      "         confidentiality = True             spam : ham    =     48.3 : 1.0\n",
      "                  kindly = True             spam : ham    =     46.9 : 1.0\n",
      "                     sir = True             spam : ham    =     44.9 : 1.0\n",
      "                   opted = True             spam : ham    =     44.9 : 1.0\n",
      "                      pa = True             spam : ham    =     44.9 : 1.0\n",
      "              profitable = True             spam : ham    =     42.1 : 1.0\n",
      "             subscribers = True             spam : ham    =     41.6 : 1.0\n",
      "              subscriber = True             spam : ham    =     41.6 : 1.0\n",
      "              instructed = True             spam : ham    =     41.6 : 1.0\n",
      "                  postal = True             spam : ham    =     41.6 : 1.0\n",
      "                      mo = True             spam : ham    =     41.6 : 1.0\n",
      "              assistance = True             spam : ham    =     40.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.show_most_informative_features(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算字詞關聯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "appledaily = pandas.read_excel('https://raw.githubusercontent.com/ywchiu/fuboni/master/data/appledaily20171214.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.221 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "corpus = []\n",
    "titles = []\n",
    "for rec in appledaily.iterrows():\n",
    "    title = rec[1].title\n",
    "    content = rec[1].content\n",
    "    titles.append(title)\n",
    "    corpus.append(' '.join(jieba.cut(content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<899x38658 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 128159 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<38658x899 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 128159 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cs = cosine_similarity(X.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "features = np.array(vectorizer.get_feature_names())\n",
    "pos = cs[features == '曾銘宗'][0].argsort()[::-1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['曾銘宗', '保代', '純屬子', '中生', '銀金庫', '慶富關', '出具', '該聯', '參貸', '人個'], \n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Gensim 產生同義詞\n",
    "- ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.4.0-cp36-cp36m-win_amd64.whl (22.5MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.5.6.tar.gz\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading boto3-1.6.3-py2.py3-none-any.whl (128kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.6,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.22,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "Collecting botocore<1.10.0,>=1.9.3 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading botocore-1.9.3-py2.py3-none-any.whl (4.1MB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.10.0,>=1.9.3->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.10.0,>=1.9.3->boto3->smart-open>=1.2.1->gensim)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open: started\n",
      "  Running setup.py bdist_wheel for smart-open: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\User\\AppData\\Local\\pip\\Cache\\wheels\\36\\48\\35\\97efc2bd1b233627131c9a936c9de23681846db707b907d353\n",
      "  Running setup.py bdist_wheel for bz2file: started\n",
      "  Running setup.py bdist_wheel for bz2file: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\User\\AppData\\Local\\pip\\Cache\\wheels\\31\\9c\\20\\996d65ca104cbca940b1b053299b68459391c01c774d073126\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.6.3 botocore-1.9.3 bz2file-0.98 gensim-3.4.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.5.6\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?word2vec.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "appledaily = pandas.read_excel('https://raw.githubusercontent.com/ywchiu/fuboni/master/data/appledaily20171214.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "corpus = []\n",
    "for rec in appledaily.iterrows():\n",
    "    content = rec[1].content\n",
    "    corpus.append(list(jieba.cut(content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 10\n",
    "min_word_count = 10\n",
    "num_workers = 4\n",
    "context = 10\n",
    "epoch = 20\n",
    "sample = 1e-5\n",
    "model = word2vec.Word2Vec(corpus, workers = num_workers,\n",
    "                          sample = sample,\n",
    "                          size = num_features,\n",
    "                          min_count=min_word_count,\n",
    "                          window = context,\n",
    "                          iter = epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['新增', '：', '立委', '說', '黨', '立法院', '黨團', '預計', '在', '明天']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.wv.vocab)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('何守', 0.9994602203369141),\n",
       " ('~', 0.9986851215362549),\n",
       " ('跟', 0.9985789060592651),\n",
       " ('媽媽', 0.9983610510826111),\n",
       " ('她', 0.9983365535736084),\n",
       " ('離婚', 0.9983351826667786),\n",
       " ('提告', 0.9983201026916504),\n",
       " ('不管', 0.9981450438499451),\n",
       " ('馬', 0.9978952407836914),\n",
       " ('健身房', 0.9978594183921814)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('小嫻')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
